# 1 网络结构
网络结构主要由以下几部分组成：
* **Backbone**: $\color{red}{New CSP-Darknet53}$
* **Neck**: $\color{red}{SPPF, New CSP-PAN}$
* **Head**: $\color{red}{YOLOv3 Head}$

![在这里插入图片描述](https://img-blog.csdnimg.cn/870b5d3ddd4245ada97160926ec1b2d6.png)

下面是根据**yolov5l.yaml**绘制的网络整体结构，YOLOv5针对不同大小**（n, s, m, l, x）**的网络整体架构都是一样的，只不过会在每个子模块中采用不同的深度和宽度，分别应对yaml文件中的**depth_multiple**和**width_multiple**参数。还需要注意一点，官方除了**n, s, m, l, x**版本外还有**n6, s6, m6, l6, x6**，区别在于后者是针对更大分辨率的图片比如**1280x1280**，当然结构上也有些差异，后者会下采样64倍，采用4个预测特征层，而前者只会下采样到32倍且采用3个预测特征层。本文只讨论前者。
![在这里插入图片描述](https://img-blog.csdnimg.cn/528b42f73c5243f5aa3fa61d98983ee8.png#pic_center)

## 1.1 Backbone
通过和YOLOv4对比，其实YOLOv5在Backbone部分没太大变化。但是YOLOv5在v6.0版本后相比之前版本有一个很小的改动，把网络的第一层（原来是Focus模块）换成了一个6x6大小的卷积层。两者在理论上其实等价的，但是对于现有的一些GPU设备（以及相应的优化算法）使用6x6大小的卷积层比使用Focus模块更加高效。详情可以参考这个issue #4825。下图是原来的Focus模块(和之前Swin Transformer中的Patch Merging类似)，将每个2x2的相邻像素划分为一个patch，然后将每个patch中相同位置（同一颜色）像素给拼在一起就得到了4个feature map，然后在接上一个3x3大小的卷积层。这和直接使用一个6x6大小的卷积层等效。
### （1）Focus结构
![在这里插入图片描述](https://img-blog.csdnimg.cn/d30a98fd986a4516b31f7d9af38a4291.png#pic_center)
Focus结构，在Yolov3&Yolov4中并没有这个结构，其中比较关键是切片操作。
比如右图的切片示意图，4*4*3的图像切片后变成2*2*12的特征图。
需要注意的是：Yolov5s的Focus结构最后使用了32个卷积核，而其他三种结构，使用的数量有所增加。
### （2）New CSP-Darknet53
![在这里插入图片描述](https://img-blog.csdnimg.cn/2909fff88793406eb3ba39b27850c781.png)
而Yolov5中设计了两种CSP结构，以Yolov5s网络为例，应用于Backbone主干网络的CSP结构中Bottleneck模块**用到**了残差模块，应用于Neck中的CSP结构中Bottleneck模块则**没有用到**了残差模块。
## 1.2 Neck
### （1）FPN+PAN
![在这里插入图片描述](https://img-blog.csdnimg.cn/76d62e7492e24456871f4961d9b7ac31.png)
但如上面CSPNet结构中讲到，Yolov5和Yolov4的不同点在于，
Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP结构（不用残差模块），加强网络特征融合的能力。
![在这里插入图片描述](https://img-blog.csdnimg.cn/541c435588484975b21be12967338280.png)
### （2）SPPF
将SPP换成成了SPPF（Glenn Jocher自己设计的），两者的作用是一样的，但后者效率更高。SPP结构如下图所示，是将输入并行通过多个不同大小的MaxPool，然后做进一步融合，能在一定程度上解决目标多尺度问题。
![在这里插入图片描述](https://img-blog.csdnimg.cn/1922d547c3204af4998ed57bc026faad.png)
而SPPF结构是将输入串行通过多个5x5大小的MaxPool层，这里需要注意的是串行两个5x5大小的MaxPool层是和一个9x9大小的MaxPool层计算结果是一样的，串行三个5x5大小的MaxPool层是和一个13x13大小的MaxPool层计算结果是一样的。
![在这里插入图片描述](https://img-blog.csdnimg.cn/be6309e513f646e78d1f6eddeb60d0e1.png)
下面做个简单的小实验，对比下SPP和SPPF的计算结果以及速度，代码如下（注意这里将SPPF中最开始和结尾处的1x1卷积层给去掉了，只对比含有MaxPool的部分）：

```python
import time
import torch
import torch.nn as nn


class SPP(nn.Module):
    def __init__(self):
        super().__init__()
        self.maxpool1 = nn.MaxPool2d(5, 1, padding=2)
        self.maxpool2 = nn.MaxPool2d(9, 1, padding=4)
        self.maxpool3 = nn.MaxPool2d(13, 1, padding=6)

    def forward(self, x):
        o1 = self.maxpool1(x)
        o2 = self.maxpool2(x)
        o3 = self.maxpool3(x)
        return torch.cat([x, o1, o2, o3], dim=1)


class SPPF(nn.Module):
    def __init__(self):
        super().__init__()
        self.maxpool = nn.MaxPool2d(5, 1, padding=2)

    def forward(self, x):
        o1 = self.maxpool(x)
        o2 = self.maxpool(o1)
        o3 = self.maxpool(o2)
        return torch.cat([x, o1, o2, o3], dim=1)


def main():
    input_tensor = torch.rand(8, 32, 16, 16)
    spp = SPP()
    sppf = SPPF()
    output1 = spp(input_tensor)
    output2 = sppf(input_tensor)

    print(torch.equal(output1, output2))

    t_start = time.time()
    for _ in range(100):
        spp(input_tensor)
    print(f"spp time: {time.time() - t_start}")

    t_start = time.time()
    for _ in range(100):
        sppf(input_tensor)
    print(f"sppf time: {time.time() - t_start}")


if __name__ == '__main__':
    main()
```
终端输出：

```python
True
spp time: 0.5373051166534424
sppf time: 0.20780706405639648
```

通过对比可以发现，两者的计算结果是一模一样的，但SPPF比SPP计算速度快了不止两倍，快乐翻倍。
## 1.3 Prediction
### （1）CIOU_loss
目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。
Bounding Box Regeression的Loss近些年的发展过程是：Smooth L1 Loss-> IoU Loss（2016）-> GIoU Loss（2019）-> DIoU Loss（2020）->CIoU Loss（2020）
我们从最常用的IOU_Loss开始，进行对比拆解分析，看下Yolov5为啥要选择CIOU_Loss。
#### a.IOU_Loss
![在这里插入图片描述](https://img-blog.csdnimg.cn/0532aeae55e74aa790a1f3faf3fc16e8.png)
可以看到IOU的loss其实很简单，主要是交集/并集，但其实也存在两个问题。
![在这里插入图片描述](https://img-blog.csdnimg.cn/64659295f6d24500a2ec49ef2f3fa833.png)
**问题1**：即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。
**问题2**：即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。
因此2019年出现了GIOU_Loss来进行改进。
#### b.GIOU_Loss
![在这里插入图片描述](https://img-blog.csdnimg.cn/6d29f5aff3394a01b1514ee8c0270f1f.png)
可以看到右图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。
但为什么仅仅说缓解呢？
因为还存在一种不足：
![在这里插入图片描述](https://img-blog.csdnimg.cn/e0da9501e85d41299f9658e3c3c9dba4.png)
**问题**：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。
基于这个问题，2020年的AAAI又提出了DIOU_Loss。
#### c.DIOU_Loss
好的目标框回归函数应该考虑三个重要几何因素：**重叠面积、中心点距离，长宽比**。

针对IOU和GIOU存在的问题，作者从两个方面进行考虑
**一：如何最小化预测框和目标框之间的归一化距离？**

**二：如何在预测框和目标框重叠时，回归的更准确？**
针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）
![在这里插入图片描述](https://img-blog.csdnimg.cn/eeffb0d8948f47a0b82d1b733afd65e2.png)
但就像前面好的目标框回归函数所说的，没有考虑到长宽比。
![在这里插入图片描述](https://img-blog.csdnimg.cn/8007e6f4161b4c438172d3f711b1db21.png)
比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。
但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。
针对这个问题，又提出了CIOU_Loss，不对不说，科学总是在解决问题中，不断进步！！
#### d.CIOU_Loss
CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。
![在这里插入图片描述](https://img-blog.csdnimg.cn/599853124a34428b81ee4aa6b90983e4.png)
其中v是衡量长宽比一致性的参数，我们也可以定义为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/ee1383d339074ce6aa36c5c83258145f.png)
这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。
>再来综合的看下各个Loss函数的不同点：
>**IOU_Loss**：主要考虑检测框和目标框重叠面积。
>**GIOU_Loss**：在IOU的基础上，解决边界框不重合时的问题。
>**DIOU_Loss**：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。
>**CIOU_Loss**：在DIOU的基础上，考虑边界框宽高比的尺度信息。
>Yolov5中采用了**CIOU_Loss**的回归方式，使得预测框回归的速度和精度更高一些。
### (2) DIOU_nms
在目标检测的后处理过程中，针对很多目标框的筛选，通常需要nms操作。
因为CIOU_Loss中包含影响因子v，涉及groudtruth的信息，而测试推理时，是没有groundtruth的。
所以Yolov4在DIOU_Loss的基础上采用DIOU_nms的方式，而Yolov5中采用加权nms的方式。
可以看出，采用DIOU_nms，下方中间箭头的黄色部分，原本被遮挡的摩托车也可以检出。
![在这里插入图片描述](https://img-blog.csdnimg.cn/6b88ca9d16be4b5c9081a0abc0a99552.png)
## 2 数据增强
### 1. 对原图做数据增强
* 像素级：HSV增强、旋转、缩放、平移、剪切、透视、翻转等
* 图片级：MixUp、Cutout、CutMix、Mosaic、Copy-Paste(Segment)等

### 2. 对标签做同样的增强
* 变换后的坐标偏移量
* 防止标签坐标越界

>Mosaic数据增强
Mosaic数据增强在YOLOv4就已经被使用，与**CutMix**有一定的相似性。**Mosaic**利用了四张图片，对四张图片进行随机拼接（**随机缩放、随机裁剪、随机排布**），每一张图片都有其对应的GT框，将四张图片拼接之后就获得一张新的图片，同时也获得这张图片对应的GT框，然后我们将这样一张新的图片传入到神经网络当中去训练，这样就极大地丰富了检测物体背景，并且在BN计算的时候会直接计算四张图片。
![在这里插入图片描述](https://img-blog.csdnimg.cn/1e4eb78feb9e44c6938168fce07605dc.png)
>代码主要流程如下：
>* Step1：假设模型输入尺寸为s，首先初始化一幅尺寸为2s*2s的灰色大图
>* Step2：在大图中从点A（s/2, s/2）和点B（3s/2, 3s/2）限定的矩形内随机选择一点作为拼接点
>* Step3：随机选择四张图，取其部分拼入大图，超出的部分将被舍弃
>* Step4：根据原图坐标的偏移量，重新计算GT框的坐标，并使用np.clip防止更新后的标签坐标越界


## 3 输入端优化
### 3.1自适应锚框计算
在Yolo算法中，针对不同的数据集，都会有初始设定长宽的锚框。
在网络训练中，网络在初始锚框的基础上输出预测框，进而和真实框groundtruth进行比对，计算两者差距，再反向更新，迭代网络参数。
因此初始锚框也是比较重要的一部分，比如Yolov5在Coco数据集上初始设定的锚框：

图片

在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的。
但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。
当然，如果觉得计算的锚框效果不是很好，也可以在代码中将自动计算锚框功能关闭。
![在这里插入图片描述](https://img-blog.csdnimg.cn/87dadb6e4d0b45d9afe7a0f293f13b1d.png)
控制的代码即train.py中上面一行代码，设置成False，每次训练时，不会自动计算。
### 3.2自适应图片缩放
在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。
比如Yolo算法中常用416*416，608*608等尺寸，比如对下面800*600的图像进行缩放。
![在这里插入图片描述](https://img-blog.csdnimg.cn/b1a4caec649449fbb0a3b532032c0182.png)
但Yolov5代码中对此进行了改进，也是Yolov5推理速度能够很快的一个不错的trick。
作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。
因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。
![在这里插入图片描述](https://img-blog.csdnimg.cn/0f410b08dd4745c49a301d12300b3854.png)
图像高度上两端的黑边变少了，在推理时，计算量也会减少，即目标检测速度会得到提升。
在datasets.py的letterbox函数中有详细的代码。
第一步：计算缩放比例
![在这里插入图片描述](https://img-blog.csdnimg.cn/92c1acf9534e4804b45c4e37f46b2625.png)
原始缩放尺寸是416*416，都除以原始图像的尺寸后，可以得到0.52，和0.69两个缩放系数，选择小的缩放系数。
第二步：计算缩放后的尺寸
![在这里插入图片描述](https://img-blog.csdnimg.cn/43c6cc2dcb4f415294b1435a2d4f05da.png)
原始图片的长宽都乘以最小的缩放系数0.52，宽变成了416，而高变成了312。
第三步：计算黑边填充数值
![在这里插入图片描述](https://img-blog.csdnimg.cn/0a03a72e422d4277b5480b5adadffe28.png)
将416-312=104，得到原本需要填充的高度。再采用numpy中np.mod取余数的方式，得到8个像素，再除以2，即得到图片高度两端需要填充的数值。
此外，需要注意的是：

a.这里大白填充的是黑色，即（0，0，0），而Yolov5中填充的是灰色，即（114,114,114），都是一样的效果。

b.训练时没有采用缩减黑边的方式，还是采用传统填充的方式，即缩放到416*416大小。只是在测试，使用模型推理时，才采用缩减黑边的方式，提高目标检测，推理的速度。

c.为什么np.mod函数的后面用32？因为Yolov5的网络经过5次下采样，而2的5次方，等于32。所以至少要去掉32的倍数，再进行取余。


## 3 Yolov5四种网络结构的不同点
Yolov5代码中的四种网络，和之前的Yolov3，Yolov4中的cfg文件不同，都是以yaml的形式来呈现。
而且四个文件的内容基本上都是一样的，只有最上方的depth_multiple和width_multiple两个参数不同，很多同学看的一脸懵逼，不知道只通过两个参数是如何控制四种结构的？
### 3.1 四种结构的参数
#### （1）Yolov5s.yaml
![在这里插入图片描述](https://img-blog.csdnimg.cn/acf03e5f765648f3b63a5600c3bf7363.png)
#### （2）Yolov5m.yaml
![在这里插入图片描述](https://img-blog.csdnimg.cn/c1c84154aa7147dcbb8fc5a7c3d9a5c2.png)
#### （3）Yolov5l.yaml
![在这里插入图片描述](https://img-blog.csdnimg.cn/dc43768675494f6eadb5b29e781424e7.png)

#### （4）Yolov5x.yaml
![在这里插入图片描述](https://img-blog.csdnimg.cn/88cc182822a545b38b65cc3d07a61a69.png)

>四种结构就是通过上面的两个参数，来进行控制网络的**深度**和**宽度**。其中**depth_multiple**控制网络的**深度**，**width_multiple**控制网络的**宽度**。

### 3.2 Yolov5网络结构
四种结构的yaml文件中，下方的网络架构代码都是一样的。
为了便于讲解，将其中的Backbone部分提取出来，讲解如何控制网络的宽度和深度，yaml文件中的Head部分也是同样的原理。
![在这里插入图片描述](https://img-blog.csdnimg.cn/4b598ede3a3243f7bd1db9878f3fef2a.png)
在对网络结构进行解析时，yolo.py中下方的这一行代码将四种结构的depth_multiple，width_multiple提取出，赋值给gd，gw。后面主要对这gd，gw这两个参数进行讲解。
![在这里插入图片描述](https://img-blog.csdnimg.cn/7d97a2bbc07c480381b26b925a0c2d84.png)
下面再细致的剖析下，看是如何控制每种结构，深度和宽度的。
### 3.3 Yolov5四种网络的深度
![在这里插入图片描述](https://img-blog.csdnimg.cn/81f6264da5494232ae19732d798b4549.png)
#### （1）不同网络的深度
在上图中，有两种CSP结构，CSP1和CSP2，其中CSP1结构主要应用于Backbone中，CSP2结构主要应用于Neck中。
**需要注意的是，四种网络结构中每个CSP结构的深度都是不同的。**
* a.以yolov5s为例，第一个CSP1中，使用了1个残差组件，因此是**CSP1_1**。而在Yolov5m中，则增加了网络的深度，在第一个CSP1中，使用了2个残差组件，因此是**CSP1_2**。
而Yolov5l中，同样的位置，则使用了**3个残差组件**，Yolov5x中，使用了**4个残差组件**。
其余的第二个CSP1和第三个CSP1也是同样的原理。
* b.在第二种CSP2结构中也是同样的方式，以第一个CSP2结构为例，Yolov5s组件中使用了2×X=2×1=2个卷积，因为Ｘ=1，所以使用了1组卷积，因此是CSP2_1。
而Yolov5m中使用了**2组**，Yolov5l中使用了**3组**，Yolov5x中使用了**4组**。
其他的四个CSP2结构，也是同理。
Yolov5中，网络的不断加深，也在不断**增加网络特征提取和特征融合的能力**。

#### （2）控制深度的代码
控制四种网络结构的核心代码是**yolo.py**中下面的代码，存在两个变量，**n**和**gd**。
我们再将**n**和**gd**带入计算，看每种网络的变化结果。
![在这里插入图片描述](https://img-blog.csdnimg.cn/329ec199ef9845258ea30c15710697dc.png)
#### （3）验证控制深度的有效性
我们选择最小的**yolov5s.yaml**和中间的**yolov5l.yaml**两个网络结构，将**gd**(**depth_multiple**)系数带入，看是否正确。
![在这里插入图片描述](https://img-blog.csdnimg.cn/2783609ac8394a0a85892cda0ea8cf92.png)
##### a. yolov5s.yaml
其中**depth_multiple=0.33**，即**gd=0.33**，而**n**则由上面红色框中的信息获得。
以上面网络框图中的第一个CSP1为例，即上面的第一个红色框。**n**等于第二个数值3。
而**gd=0.33**，带入（2）中的计算代码，结果**n=1**。因此第一个CSP1结构内只有1个残差组件，即CSP1_1。
第二个CSP1结构中，n等于第二个数值9，而**gd=0.33**，带入（2）中计算，结果**n=3**，因此第二个CSP1结构中有3个残差组件，即CSP1_3。
第三个CSP1结构也是同理，这里不多说。
##### b. yolov5l.xml
其中**depth_multiple=1**，即**gd=1**
和上面的计算方式相同，第一个CSP1结构中，n=3，带入代码中，结果n=3，因此为CSP1_3。
下面第二个CSP1和第三个CSP1结构都是同样的原理。
### 3.4 Yolov5四种网络的宽度
![在这里插入图片描述](https://img-blog.csdnimg.cn/d8ac6555246446fd856ec72c96e19a76.png)
#### （1）不同网络的宽度:
如上图表格中所示，四种yolov5结构在不同阶段的卷积核的数量都是不一样的，因此也直接影响卷积后特征图的第三维度，即**厚度**，这里表示为网络的**宽度**。
* a.以Yolov5s结构为例，第一个Focus结构中，最后卷积操作时，卷积核的数量是**32**个，因此经过Focus结构，特征图的大小变成**304 * 304 * 32**。
而yolov5m的Focus结构中的卷积操作使用了48个卷积核，因此Focus结构后的特征图变成**304* 304*48**。yolov5l，yolov5x也是同样的原理。
* b. 第二个卷积操作时，yolov5s使用了64个卷积核，因此得到的特征图是**152 * 152 * 64**。而yolov5m使用96个特征图，因此得到的特征图是**152* 152*96**。yolov5l，yolov5x也是同理。
* c. 后面三个卷积下采样操作也是同样的原理。

四种不同结构的卷积核的数量不同，这也直接影响网络中，比如CSP1，CSP2等结构，以及各个普通卷积，卷积操作时的卷积核数量也同步在调整，影响整体网络的计算量。
当然卷积核的数量越多，特征图的厚度，即**宽度越宽**，网络提取特征的学习能力也越强。

#### （2）控制宽度的代码
在yolov5的代码中，控制宽度的核心代码是**yolo.py**文件里面的这一行：
![在这里插入图片描述](https://img-blog.csdnimg.cn/110b5284595b4e1d85d370d1edd799c7.png)
#### （3）验证控制宽度的有效性
我们还是选择**最小的yolov5s**和**中间的yolov5l**两个网络结构，将width_multiple系数带入，看是否正确。
![在这里插入图片描述](https://img-blog.csdnimg.cn/d1f6e24d55a1465ab02139dd73d699ee.png)
#### a. yolov5s.yaml
其中**width_multiple=0.5**，即**gw=0.5**。
![在这里插入图片描述](https://img-blog.csdnimg.cn/8b6519bbbb174f858ea94228b097ab6f.png)

以第一个卷积下采样为例，即Focus结构中下面的卷积操作。
按照上面Backbone的信息，我们知道Focus中，标准的c2=64，而**gw=0.5**，代入（2）中的计算公式，最后的结果**=32**。即Yolov5s的Focus结构中，卷积下采样操作的卷积核数量为**32个**。
再计算后面的第二个卷积下采样操作，标准c2的值=128，**gw=0.5**，代入（2）中公式，最后的结果**=64**，也是正确的。
#### b. yolov5l.yaml
其中**width_multiple=1**，即**gw=1**，而标准的**c2=64**，代入上面（2）的计算公式中，可以得到Yolov5l的Focus结构中，卷积下采样操作的卷积核的数量为**64个**，而第二个卷积下采样的卷积核数量是**128个**。
另外的三个卷积下采样操作，以及yolov5m，yolov5x结构也是同样的计算方式。

## 参考：
https://blog.csdn.net/weixin_43799388/article/details/123830587
https://zhuanlan.zhihu.com/p/172121380
